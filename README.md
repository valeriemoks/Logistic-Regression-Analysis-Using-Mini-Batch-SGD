# Logistic-Regression-Analysis-Using-Mini-Batch-SGD
This project explores how different weight initialization methods, batch sizes, and learning rates affect the performance of a logistic regression model trained with mini-batch stochastic gradient descent (SGD). Using a dataset of Gaussian-distributed samples, we found that initializing weights to zero or with a uniform random distribution yielded identical training loss (6.671e-06) and perfect testing accuracy (100%), indicating no significant impact from the initialization method. Comparisons of gradient descent variants showed that smaller batch sizes (2 and 32) converged faster but were more computationally intensive, while larger batch sizes (256 and 512) converged slower. Additionally, higher learning rates (0.2 and 0.3) sped up convergence but risked divergence, with an optimal learning rate of 0.2 providing the best balance. These findings underscore the importance of tuning hyperparameters for efficient and effective model training.
